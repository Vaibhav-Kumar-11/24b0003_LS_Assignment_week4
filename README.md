# ğŸ§  Next-Word Predictor using GPT-2

Welcome to my **final project** for the NLP course!  
This project is a simple and elegant implementation of a **Next-Word Prediction system** using the powerful GPT-2 model from Hugging Face Transformers.  
The model takes a user-provided prompt and predicts how the sentence is likely to continue âœ¨.

---

## ğŸ“‹ Project Overview

> **Goal:** Build a text generation model that can predict the next word(s) given an input sentence or phrase.

This project focuses on:
- ğŸ” Tokenizing and formatting a real-world dataset (`WikiText-2`)
- ğŸ§  Fine-tuning the GPT-2 language model
- ğŸ› ï¸ Implementing a next-word prediction function
- ğŸ—£ï¸ Generating natural text based on a seed input

---

## ğŸ› ï¸ Tech Stack

- **Python** ğŸ
- **Hugging Face Transformers** ğŸ¤—
- **Datasets Library** ğŸ“¦
- **PyTorch** ğŸ”¥
- **VS Code / Jupyter** ğŸ’»

---

## ğŸ“š Dataset Used

The model is trained using the **WikiText-2** dataset:
- ğŸ“– Clean and curated Wikipedia text
- âœ‚ï¸ Limited to the first 1000 lines for fast, beginner-friendly training
- ğŸš€ Lightweight enough to run on a CPU

---

## ğŸ§ª Model Training Steps

```python
# Step-by-step process:
1. Load Dataset         â†’ `load_dataset("wikitext", "wikitext-2-raw-v1")`
2. Tokenize Text        â†’ using `AutoTokenizer` from GPT-2
3. Format Input Blocks  â†’ fixed length using a grouping function
4. Load GPT-2 Model     â†’ `GPT2LMHeadModel.from_pretrained("gpt2")`
5. Train Model          â†’ with `Trainer` API
6. Predict Next Words   â†’ using `.generate()` function

ğŸ¤– Sample Prediction:
Input  â¤ "The future of artificial intelligence is"
Output â¤ "The future of artificial intelligence is not just promising but inevitable in shaping our world."
It continues your sentence in a realistic and fluent way ğŸ’¬

ğŸš€ How to Run:

âœ… Install Dependencies
bash
Copy
Edit
pip install transformers datasets torch

âœ… Run the Script
Copy the code from main.py or run the .ipynb notebook.

Use the predict_next_word() function to generate completions.

ğŸ“ˆ Customizations
Feel free to:

Use larger GPT-2 models (gpt2-medium, gpt2-large)

Expand dataset or epochs for better results

Add a Streamlit/Gradio interface for real-time generation


ğŸ§  What I Learned:
How transformer models tokenize and understand language

Practical usage of Hugging Face Trainer API

Preprocessing and batching real datasets

Building user-facing functions for intelligent output

ğŸ™‹ Author Info
Name: [Your Name]
Institute: IIT Bombay
Course Track: NLP (Transformers Track)
Project: Final Submission â€” Week 4

â­ Final Words
This project marks the completion of my NLP course journey â€” starting from scratch to building a working text generation model with real AI ğŸ”¥

If you're curious, experiment and build â€” youâ€™ll learn more than you ever expect! ğŸš€

